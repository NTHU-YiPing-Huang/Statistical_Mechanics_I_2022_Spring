
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <meta name="lang:clipboard.copy" content="Copy to clipboard">
  <meta name="lang:clipboard.copied" content="Copied to clipboard">
  <meta name="lang:search.language" content="en">
  <meta name="lang:search.pipeline.stopwords" content="True">
  <meta name="lang:search.pipeline.trimmer" content="True">
  <meta name="lang:search.result.none" content="No matching documents">
  <meta name="lang:search.result.one" content="1 matching document">
  <meta name="lang:search.result.other" content="# matching documents">
  <meta name="lang:search.tokenizer" content="[\s\-]+">

  
    <link href="https://fonts.gstatic.com/" rel="preconnect" crossorigin>
    <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,500,700|Roboto:300,400,400i,700&display=fallback" rel="stylesheet">

    <style>
      body,
      input {
        font-family: "Roboto", "Helvetica Neue", Helvetica, Arial, sans-serif
      }

      code,
      kbd,
      pre {
        font-family: "Roboto Mono", "Courier New", Courier, monospace
      }
    </style>
  

  <link rel="stylesheet" href="_static/stylesheets/application.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-palette.css"/>
  <link rel="stylesheet" href="_static/stylesheets/application-fixes.css"/>
  
  <link rel="stylesheet" href="_static/fonts/material-icons.css"/>
  
  <meta name="theme-color" content="#3f51b5">
  <script src="_static/javascripts/modernizr.js"></script>
  
  
  
    <title>Basic concepts in probability theory &#8212; 2022 Statistical Mechanics (I) - PHYS521000</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/material.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Thermodynamics and statistical mechanics" href="Rev_thermodynamics.html" />
    <link rel="prev" title="The diffusion equation" href="diffusion_eq.html" />
  
   

  </head>
  <body dir=ltr
        data-md-color-primary=blue-grey data-md-color-accent=blue>
  
  <svg class="md-svg">
    <defs data-children-count="0">
      
      <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
      
    </defs>
  </svg>
  
  <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer">
  <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search">
  <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
  <a href="#basic_prob" tabindex="1" class="md-skip"> Skip to content </a>
  <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex navheader">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="syllabus.html" title="2022 Statistical Mechanics (I) - PHYS521000"
           class="md-header-nav__button md-logo">
          
              <img src="_static/logo-wide.svg" height="26"
                   alt="2022 Statistical Mechanics (I) - PHYS521000 logo">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          <span class="md-header-nav__topic">2022 Statistical Mechanics (I) - PHYS521000</span>
          <span class="md-header-nav__topic"> Basic concepts in probability theory </span>
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
        
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" action="search.html" method="get" name="search">
      <input type="text" class="md-search__input" name="q" placeholder="Search"
             autocapitalize="off" autocomplete="off" spellcheck="false"
             data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>

      </div>
      
      
  
  <script src="_static/javascripts/version_dropdown.js"></script>
  <script>
    var json_loc = ""versions.json"",
        target_loc = "../",
        text = "Versions";
    $( document ).ready( add_version_dropdown(json_loc, target_loc, text));
  </script>
  

    </div>
  </nav>
</header>

  
  <div class="md-container">
    
    
    
  <nav class="md-tabs" data-md-component="tabs">
    <div class="md-tabs__inner md-grid">
      <ul class="md-tabs__list">
          <li class="md-tabs__item"><a href="syllabus.html" class="md-tabs__link">2022 Statistical Mechanics (I) - PHYS521000</a></li>
          <li class="md-tabs__item"><a href="random_walk_and_emergent_properties.html" class="md-tabs__link">Random walk and emergent properties</a></li>
      </ul>
    </div>
  </nav>
    <main class="md-main">
      <div class="md-main__inner md-grid" data-md-component="container">
        
          <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="syllabus.html" title="2022 Statistical Mechanics (I) - PHYS521000" class="md-nav__button md-logo">
      
        <img src="_static/logo-wide.svg" alt=" logo" width="48" height="48">
      
    </a>
    <a href="syllabus.html"
       title="2022 Statistical Mechanics (I) - PHYS521000">2022 Statistical Mechanics (I) - PHYS521000</a>
  </label>
  

</nav>
              </div>
            </div>
          </div>
          <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
            <div class="md-sidebar__scrollwrap">
              <div class="md-sidebar__inner">
                
<nav class="md-nav md-nav--secondary">
    <label class="md-nav__title" for="__toc">Contents</label>
  <ul class="md-nav__list" data-md-scrollfix="">
        <li class="md-nav__item"><a href="#basic-prob--page-root" class="md-nav__link">Basic concepts in probability theory</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#simple-notions" class="md-nav__link">Simple notions</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#what-is-probability" class="md-nav__link">What is probability?</a>
        </li>
        <li class="md-nav__item"><a href="#probability-of-multiple-variables" class="md-nav__link">Probability of multiple variables</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#joint-probability" class="md-nav__link">Joint probability</a>
        </li>
        <li class="md-nav__item"><a href="#margin-probability" class="md-nav__link">Margin probability</a>
        </li>
        <li class="md-nav__item"><a href="#conditional-probability" class="md-nav__link">Conditional probability</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#independent-variables" class="md-nav__link">Independent variables</a>
        </li>
        <li class="md-nav__item"><a href="#mean-variance-and-standard-deviation" class="md-nav__link">Mean, variance and standard deviation</a>
        </li>
        <li class="md-nav__item"><a href="#gaussian-integral" class="md-nav__link">Gaussian integral</a><nav class="md-nav">
              <ul class="md-nav__list">
        <li class="md-nav__item"><a href="#one-dimensional-gaussian-integral" class="md-nav__link">One-dimensional Gaussian integral.</a>
        </li>
        <li class="md-nav__item"><a href="#multidimensional-gaussian-integral" class="md-nav__link">Multidimensional Gaussian integral</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#stirling-s-approximation" class="md-nav__link">Stirling’s approximation</a>
        </li></ul>
            </nav>
        </li>
        <li class="md-nav__item"><a href="#sets-of-independent-random-numbers" class="md-nav__link">Sets of independent random numbers</a>
        </li>
        <li class="md-nav__item"><a href="#before-we-enter-statistical-mechanics" class="md-nav__link">Before we enter statistical mechanics</a>
        </li></ul>
            </nav>
        </li>
    
<li class="md-nav__item"><a class="md-nav__extra_link" href="_sources/basic_prob.md">Show Source</a> </li>

<li id="searchbox" class="md-nav__item"></li>

  </ul>
</nav>
              </div>
            </div>
          </div>
        
        <div class="md-content">
          <article class="md-content__inner md-typeset" role="main">
            
  
<h1 id="basic-prob--page-root">Basic concepts in probability theory<a class="headerlink" href="#basic-prob--page-root" title="Permalink to this headline">¶</a></h1>

<h2 id="simple-notions">Simple notions<a class="headerlink" href="#simple-notions" title="Permalink to this headline">¶</a></h2>

<h3 id="what-is-probability">What is probability?<a class="headerlink" href="#what-is-probability" title="Permalink to this headline">¶</a></h3>
<p>Suppose we have a set <span class="math notranslate nohighlight">\(\Omega\)</span> and we have several events <span class="math notranslate nohighlight">\(\omega\in\Omega\)</span>. If we randomly pick an event <span class="math notranslate nohighlight">\(\omega\)</span>, we can ask: how frequently we will get an event satisfy certain criteria <span class="math notranslate nohighlight">\(\Lambda\)</span>? The probability to have event <span class="math notranslate nohighlight">\(\lambda\in\Lambda\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\lim_{N_t\to\infty}\frac{ N(\lambda\in\Lambda)}{N_t}; N_t\text{ is the number of trials.}
\]</div>


<h3 id="probability-of-multiple-variables">Probability of multiple variables<a class="headerlink" href="#probability-of-multiple-variables" title="Permalink to this headline">¶</a></h3>
<p>In statistical mechanics, usually we need to face probability descriptions of multiple variables. We will use a simple example to demonstrate the following concepts.</p>
<p>Suppose we have <span class="math notranslate nohighlight">\(\Omega\)</span> formed by integers with color red, green and blue.</p>
<div class="math notranslate nohighlight">
\[
\Omega: \left\{Red: 1,34,7; Green: 12,5; Blue: 276, 19,18, 2331\right\}
\]</div>
<p>In addition to the color, we can also categorize the integers by its parity, <em>i.e.</em> evenness or oddness.</p>
<p>We can use these two conditions, color and parity of the integers, to illustrate the following concepts.</p>
<div class="figure align-default" id="set-relation">
<a class="reference internal image-reference" href="_images/probability.png"><img alt="_images/probability.png" src="_images/probability.png" style="width: 350px;"/></a>
<p class="caption"><span class="caption-number">Fig. 8 </span><span class="caption-text">The set <span class="math notranslate nohighlight">\(\Omega\)</span>: The circled region, <span class="math notranslate nohighlight">\(\alpha+\beta\)</span>, represent the red integers. The triangular region, <span class="math notranslate nohighlight">\(\beta+\gamma\)</span>, represent the odd integers. The set <span class="math notranslate nohighlight">\(\Omega\)</span> is <span class="math notranslate nohighlight">\(\alpha+\beta+\gamma+\delta\)</span>.</span><a class="headerlink" href="#set-relation" title="Permalink to this image">¶</a></p>
</div>

<h4 id="joint-probability">Joint probability<a class="headerlink" href="#joint-probability" title="Permalink to this headline">¶</a></h4>
<p>The joint probability ask what is the probability of two conditions to be true at the same time.
For example, we can ask what is the probability that we have red odd integers? <span class="math notranslate nohighlight">\(P(red,odd)=\frac{\beta}{\alpha+\beta+\gamma+\delta}=\frac{2}{9}\)</span>.</p>
<p>We have <span class="math notranslate nohighlight">\(P(a,b)=P(b,a)\)</span> in general.</p>
<p>The normalization condition for the joint probability is</p>
<div class="math notranslate nohighlight">
\[
\sum_{a\in A}\sum_{b\in B}P(a,b)=1\xrightarrow{i.e.}\sum_{color}\sum_{parity}P(color,parity)=1\text{.}
\]</div>


<h4 id="margin-probability">Margin probability<a class="headerlink" href="#margin-probability" title="Permalink to this headline">¶</a></h4>
<p>The margin probability means we sum over all the possibilities of one condition. For example, we can ask what is the probability to have red integer in this case. We can derive that from the joint probability and sum over the even and odd cases with color red.</p>
<div class="math notranslate nohighlight">
\[
P_A(a)=\sum_{b\in B}P(a,b)\xrightarrow{i.e.} P_{color}(Red)=\sum_{parity}P(Red,parity)=\frac{\alpha+\beta}{\alpha+\beta+\gamma+\delta}=\frac{3}{9}\text{.}
\]</div>


<h4 id="conditional-probability">Conditional probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">¶</a></h4>
<p>Conditional probability of <span class="math notranslate nohighlight">\(b\)</span> given <span class="math notranslate nohighlight">\(a\)</span> is denoted as <span class="math notranslate nohighlight">\(P(b|a)\)</span>. For example, we can ask what is the conditional probability of we pick a red integer given the integers are odd.</p>
<div class="math notranslate nohighlight">
\[
P(red|odd)=\frac{\beta}{\beta+\gamma}=\frac{2}{5}\text{.}
\]</div>
<p><span class="math notranslate nohighlight">\(P(a|b)\neq P(b|a)\)</span> in general. For example</p>
<div class="math notranslate nohighlight">
\[
P(odd|red)=\frac{\beta}{\alpha+\beta}=\frac{2}{3}\text{.}
\]</div>
<p><em>Bayes’ theorem</em></p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(a,b)=P(b,a)=P(b|a)P_A(a)=P(a|b)P_B(b)\\
\xrightarrow{i.e.} P(red,odd)=P(red|odd)P_{parity}(odd)=P(odd|red)P_{color}(red)\\
=\frac{\beta}{\beta+\gamma}\frac{\beta+\gamma}{\alpha+\beta+\gamma+\delta}=\frac{\beta}{\alpha+\beta}\frac{\alpha+\beta}{\alpha+\beta+\gamma+\delta}\text{.}
\end{split}\]</div>
<p><em>Extended form of Bayes’ theorem</em> when <span class="math notranslate nohighlight">\(\Omega\)</span> is divided into sub-regions <span class="math notranslate nohighlight">\(\Omega_i\)</span></p>
<div class="math notranslate nohighlight">
\[
P_B(b)=\sum_i P(b|\Omega_i)P(\Omega_i)\text{.}
\]</div>
<p>We can pick <span class="math notranslate nohighlight">\(\Omega_1=A\)</span> and <span class="math notranslate nohighlight">\(\Omega_2=\overline{A}\)</span>. Then we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
P(a|b)=\frac{P(b|a)P_A(a)}{P_B(b)}=\frac{P(b|a)P_A(a)}{P(b|a)P_A(a)+P(b|\overline{a})P_{\overline{A}}(\overline{a})}\\
\xrightarrow{i.e.}
P(red|odd)=\frac{P(odd|red)P_{color}(red)}{P(odd|red)P_{color}(red)+P(odd|\text{not red})P_{color}(\text{not red})}
\end{split}\]</div>



<h3 id="independent-variables">Independent variables<a class="headerlink" href="#independent-variables" title="Permalink to this headline">¶</a></h3>
<p>If <span class="math notranslate nohighlight">\(P(a,b)=P_A(a)P_B(b)\)</span>, we said <span class="math notranslate nohighlight">\(a,b\)</span> are independent variables.</p>
<p>This is because the conditional probability is <span class="math notranslate nohighlight">\(P(a|b)=\frac{P(a,b)}{P_B(b)}\xrightarrow{independent}\frac{P_A(a)P_B(b)}{P_B(b)}=P_A(a)\)</span> is independent of <span class="math notranslate nohighlight">\(b\)</span>!</p>


<h3 id="mean-variance-and-standard-deviation">Mean, variance and standard deviation<a class="headerlink" href="#mean-variance-and-standard-deviation" title="Permalink to this headline">¶</a></h3>
<p>For a random variable <span class="math notranslate nohighlight">\(a\)</span> and the function <span class="math notranslate nohighlight">\(F\)</span> will map it to another random variable <span class="math notranslate nohighlight">\(F(a)\)</span>.</p>
<ol class="simple">
<li><p>Mean: <span class="math notranslate nohighlight">\(\langle F\rangle \equiv \sum_a F(a) P_A(a)\)</span>.</p></li>
<li><p>n-th moment:<span class="math notranslate nohighlight">\(\langle F^n\rangle \equiv \sum_a [F(a)]^n P_A(a)\)</span>.</p></li>
<li><p>variance <span class="math notranslate nohighlight">\(\sigma^2=\langle (F-\langle F\rangle)^2\rangle=\sum_a \left(F(a)-\langle F\rangle\right)^2P_A(a)=\langle F^2\rangle-\langle F\rangle^2\)</span></p></li>
<li><p>correlation function between <span class="math notranslate nohighlight">\(F,G\)</span> is <span class="math notranslate nohighlight">\(f_{FG}=\langle FG\rangle-\langle F\rangle \langle G\rangle\)</span>.
That is,
<span class="math notranslate nohighlight">\( f_{FG}\equiv \sum_a\sum_b F(a)G(b)P(a,b)-\left(\sum_a F(a)P_A(a)\right)\left(\sum_bG(b)P_B(b)\right)\)</span></p></li>
</ol>


<h3 id="gaussian-integral">Gaussian integral<a class="headerlink" href="#gaussian-integral" title="Permalink to this headline">¶</a></h3>

<h4 id="one-dimensional-gaussian-integral">One-dimensional Gaussian integral.<a class="headerlink" href="#one-dimensional-gaussian-integral" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[
G_1=\int_{\infty}^{\infty}e^{-ax^2}dx\text{.}
\]</div>
<p>The trick is to square this expression, we found</p>
<div class="math notranslate nohighlight">
\[
(G_1)^2=\int_{\infty}^{\infty}\int_{\infty}^{\infty}e^{-ax^2}e^{-ay^2}dxdy\text{.}
\]</div>
<p>This integral is over a two-dimensional space; we can change the variables into polar coordinates <span class="math notranslate nohighlight">\((r,\theta)\)</span>. The angular integral and the radial integral will become trivial.</p>
<div class="math notranslate nohighlight">
\[
(G_1)^2=\int_{0}^{2\pi}\int_0^{\infty} e^{-ar^2}rdrd\theta=\frac{2\pi}{2}\int_0^\infty e^{-ar^2}2rdr=\frac{\pi}{a}\text{.}
\]</div>
<p>Once this expression is known, we can simply generalize it into the case with a linear term</p>
<div class="math notranslate nohighlight">
\[
G_1'=\int_{\infty}^{\infty}e^{-ax^2+bx}dx\text{.}
\]</div>
<p>The result will be left as an exercise.</p>


<h4 id="multidimensional-gaussian-integral">Multidimensional Gaussian integral<a class="headerlink" href="#multidimensional-gaussian-integral" title="Permalink to this headline">¶</a></h4>
<p>The multidimensional Gaussian integral is</p>
<div class="math notranslate nohighlight">
\[
G_N=\int_{\infty}^{\infty}dx_1\int_{-\infty}^{\infty}dx_2...\int_{-\infty}^{\infty}dx_N \exp\left[-\frac{1}{2} x_iA_{ij}x_j+b_ix_i\right]=\sqrt{\frac{(2\pi)^n}{\text{det} A}} \left[-\frac{b_i(A)^{-1}_{ij}b_j}{2}\right]\text{.}
\]</div>
<p>This expression can be derived by diagonalizing <span class="math notranslate nohighlight">\(A\)</span> and using the new basis to perform the independent one-dimensional integrals.</p>



<h3 id="stirling-s-approximation">Stirling’s approximation<a class="headerlink" href="#stirling-s-approximation" title="Permalink to this headline">¶</a></h3>
<p>By definition</p>
<div class="math notranslate nohighlight">
\[
N!=\int_0^{\infty}e^{-x}x^Ndx=\int_0^{\infty} e^{-x}e^{N\ln x} dx=\int_0^{\infty}e^{f(x)}dx\text{.}
\]</div>
<p>This expression is exact. However, we want to have a good approximation of the <span class="math notranslate nohighlight">\(N!\)</span> which means we want to approximate the integral at the right-hand side when <span class="math notranslate nohighlight">\(N\)</span> is large.</p>
<p>We are going to introduce the idea of the steepest descent method (a.k.a. saddle point approximation) to derive Stirling’s approximation. We will do it first and then explain why this method makes sense. Let’s find the point <span class="math notranslate nohighlight">\(x^{*}\)</span> such that <span class="math notranslate nohighlight">\(f'(x^*)=0\)</span> and expand <span class="math notranslate nohighlight">\(f(x)\)</span> around <span class="math notranslate nohighlight">\(x^{*}\)</span>.</p>
<p>We have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
f(x)= N\ln x-x=f(x^*)+f'(x^*)(x-x^*)+\frac{f''(x^*)}{2}(x-x^*)^2+...\\
f'(x)=\frac{N}{x}-1\rightarrow x^{*}=N\\
f''(x)=-\frac{N}{x^2}\rightarrow f''(x^{*})=-\frac{1}{N}\text{.}
\end{split}\]</div>
<p>Next, we can ask: does it make sense to expand around <span class="math notranslate nohighlight">\(x^*=N\)</span>? To answer that we would like to know how the error looks like? The leading order error is proportional with <span class="math notranslate nohighlight">\(f''(x^*)=-N^{-1}\)</span>. That means, when <span class="math notranslate nohighlight">\(N\)</span> is large, this error gets suppressed! The method of steepest descent is very general. However, one should analyze the error carefully. In this case, the form of <span class="math notranslate nohighlight">\(f(x)\)</span> works in our favor to suppress the leading order error! So we can approximate <span class="math notranslate nohighlight">\(N!\)</span> as</p>
<div class="math notranslate nohighlight">
\[
N!\approx \int_0^{\infty}e^{f(N)-\frac{(x-N)^2}{2N}} dx=e^{N\ln N-N} \sqrt{2\pi N}\text{.}
\]</div>
<p>This is Stirling’s approximation</p>
<div class="math notranslate nohighlight">
\[
\ln N!=N\ln N-N+\frac{1}{2}\ln(2\pi N)\text{.}
\]</div>
<p>One can insert <span class="math notranslate nohighlight">\(N=10^{23}\)</span> and see how important is each term. One can easily see the first two terms plays the dominant role in this expression.</p>
<div class="tip admonition">
<p class="admonition-title">What?</p>
<p>The method of steepest descent is a very powerful method to approximate integrals. Most of the time, we cannot evaluate integral exactly. Therefore, how to approximate the integral analytically is an important technique for analytic progress. We will refer our reader to <span id="id1">[<a class="reference internal" href="biblio.html#id5">BO99</a>]</span> for detailed discussions.</p>
</div>



<h2 id="sets-of-independent-random-numbers">Sets of independent random numbers<a class="headerlink" href="#sets-of-independent-random-numbers" title="Permalink to this headline">¶</a></h2>
<p>Suppose we have a vector <span class="math notranslate nohighlight">\(\{F_1,F_2,...,F_N\}\)</span> where <span class="math notranslate nohighlight">\(F_j\)</span> are independent random variables. In general, <span class="math notranslate nohighlight">\(F_j\)</span> is chosen from the probability distribution function <span class="math notranslate nohighlight">\(P_j(F_j)\)</span>. (Mathematically, we allow <span class="math notranslate nohighlight">\(P_j(F_j)\)</span> to be arbitrary distribution functions. However, in physical settings, usually <span class="math notranslate nohighlight">\(P_j(F_j)\)</span> is a fixed function.) This is almost equivalent to our random walk problem. Let’s try to evaluate the sum of this vector.</p>
<div class="math notranslate nohighlight">
\[
S_N=\sum_{j=1}^N F_j
\]</div>
<p>The mean of <span class="math notranslate nohighlight">\(S_N\)</span> can be expressed easily</p>
<div class="math notranslate nohighlight">
\[
\langle S_N\rangle=\sum_{j=1}^N \langle F_j\rangle \text{ (Here, we use the property that the mean is a linear operator).}
\]</div>
<p>The variance of <span class="math notranslate nohighlight">\(S_N\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sigma_{S_N}=\langle S_N^2\rangle-\langle S_N\rangle^2=\sum_{j=1,k=1}^{N}\langle F_jF_k\rangle-\left(\sum_{j=1}^N F_j\right)\left(\sum_{k=1}^N F_k\right)\\
=\sum_{j=1}^{N}\sum_{k\neq j}^{N}\langle F_jF_k\rangle+\sum_{j=1}^{N}\sum_{k=j}^{N}\langle F_jF_k\rangle-\left(\sum_{j=1}^N F_j\right)\left(\sum_{k=1}^N F_k\right)\\
=\sum_{j=1}^{N}\sum_{k\neq j}^{N}\langle F_j\rangle\langle F_k\rangle+\sum_{j=1}^{N}\langle F_j^2\rangle-\left(\sum_{j=1}^N F_j\right)\left(\sum_{k=1}^N F_k\right)\\
=\sum_{j=1}^{N}\left[\langle F_j^2\rangle-\langle F_j\rangle^2\right]=\sum_{j=1}^{N}\sigma_j^2\text{.}
\end{split}\]</div>
<p>In the third line of the equation, we use the condition that <span class="math notranslate nohighlight">\(F_j\)</span> are independent variables. That is, <span class="math notranslate nohighlight">\(\langle F_jF_k\rangle=\langle F_j\rangle\langle F_k\rangle\)</span>.</p>
<p>The above expression is for a general distribution function <span class="math notranslate nohighlight">\(P_j(F_j)\)</span>. If we are analyzing a specific physical system where <span class="math notranslate nohighlight">\(P_j(F_j)=P(F)\)</span>, we can further simplify our result to have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\langle S_N\rangle= N \langle F\rangle\\
\sigma_{S_N}=N \left[\langle F^2\rangle -\langle F\rangle^2 \right]=N\sigma^2\text{.}
\end{split}\]</div>
<p>The <em>relative standard deviation</em> is</p>
<div class="math notranslate nohighlight">
\[
\frac{\sigma_{S_N}}{\langle S_N\rangle}=\frac{1}{\sqrt{N}}\frac{\sigma}{\langle F\rangle}\text{.}
\]</div>
<p><strong>This is the most crucial result of probability theory for statistical mechanics. What does it tell us?</strong></p>
<ul class="simple">
<li><p>When <span class="math notranslate nohighlight">\(N\)</span> is large (say <span class="math notranslate nohighlight">\(N\approx 10^{23}\)</span>), the fluctuation (estimated by <span class="math notranslate nohighlight">\(\sigma_{S_N}\)</span>) is barely noticeable compared with its mean value <span class="math notranslate nohighlight">\(\langle F\rangle\)</span> since this ratio goes as <span class="math notranslate nohighlight">\(N^{-1/2}\)</span>.</p></li>
<li><p>The result is quite general; we only require the mean and the variance to be well defined. Not specific detail is required for <span class="math notranslate nohighlight">\(P(F)\)</span>.</p></li>
<li><p>We can see that the problem actually gets simpler when <span class="math notranslate nohighlight">\(N\to\infty\)</span>!</p></li>
</ul>


<h2 id="before-we-enter-statistical-mechanics">Before we enter statistical mechanics<a class="headerlink" href="#before-we-enter-statistical-mechanics" title="Permalink to this headline">¶</a></h2>
<p>Why do we spend lectures talking about statistics?</p>
<ul class="simple">
<li><p>The trivial reason: We need to those ideas for the future lectures.</p></li>
<li><p>The non-trivial reasons: it is related to two fundamental ideas of statistical mechanics.</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(N\approx 10^{23}\)</span> could lead to regular behavior! The reason is simple; the relative deviations could be suppressed by large <span class="math notranslate nohighlight">\(N\)</span>. Initially, it looks like large <span class="math notranslate nohighlight">\(N\)</span> gives us a formidable obstacle to understanding our system. However, now we realize that it could work in our favor to suppress the fluctuation.</p></li>
<li><p>The logical jump from microscopic detail description to statistical description: When <span class="math notranslate nohighlight">\(N\)</span> is large, the precise knowledge of microscopic components (<em>e.g.</em>, molecules) is inessential to understand the system as a whole. We jump from the deterministic laws of physics from the microscopic physics to a pure probability description. This postulate is bold and is not obviously true. However, instead of proving this postulate, we will see the benefits of making this postulate in the following lectures.</p></li>
</ol>
</li>
</ul>


<script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
<script>kernelName = 'python3'</script>

          </article>
        </div>
      </div>
    </main>
  </div>
  <footer class="md-footer">
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
          
            <a href="diffusion_eq.html" title="The diffusion equation"
               class="md-flex md-footer-nav__link md-footer-nav__link--prev"
               rel="prev">
              <div class="md-flex__cell md-flex__cell--shrink">
                <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
              </div>
              <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
                <span class="md-flex__ellipsis">
                  <span
                      class="md-footer-nav__direction"> Previous </span> The diffusion equation </span>
              </div>
            </a>
          
          
            <a href="Rev_thermodynamics.html" title="Thermodynamics and statistical mechanics"
               class="md-flex md-footer-nav__link md-footer-nav__link--next"
               rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span
                class="md-flex__ellipsis"> <span
                class="md-footer-nav__direction"> Next </span> Thermodynamics and statistical mechanics </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink"><i
                class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          
        </a>
        
      </nav>
    </div>
    <div class="md-footer-meta md-typeset">
      <div class="md-footer-meta__inner md-grid">
        <div class="md-footer-copyright">
          <div class="md-footer-copyright__highlight">
              &#169; Copyright 2021.
              
          </div>
            Created using
            <a href="http://www.sphinx-doc.org/">Sphinx</a> 3.5.4.
             and
            <a href="https://github.com/bashtage/sphinx-material/">Material for
              Sphinx</a>
        </div>
      </div>
    </div>
  </footer>
  <script src="_static/javascripts/application.js"></script>
  <script>app.initialize({version: "1.0.4", url: {base: ".."}})</script>
  </body>
</html>